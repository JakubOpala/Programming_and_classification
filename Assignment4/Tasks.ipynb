{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import * #text1, text2, text3, text4, text5, text6, text7, text8, text9\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Function takes as an input string, removes punctuation, stopwords and reduces inflected words to their stems,\n",
    "    then returns a list of the remaining words.\n",
    "\n",
    "    Parameters:\n",
    "            text(String): initial string\n",
    "\n",
    "    Return:\n",
    "            cleared(List[String]): a list consisting of every word from input text without punctuation,\n",
    "                                   stopwords and inflected words reduced to their stems\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "    tokens = tokenizer.tokenize(' '.join([token for token in text]))\n",
    "\n",
    "    nopunct_text = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    no_stopwords = [token for token in nopunct_text if token.lower() not in stop_words]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in no_stopwords]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "texts = [remove(txt) for txt in [text1, text2, text3, text4, text5, text6, text7, text8, text9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640844019907389903271413642195655937062321 ['0101100000010001100101110011011101111011001100010001011001001000000111001111011111000010111010010111', '1010000010100000001110010101010001110011011010010111110111111001011011101100011101100111110011101001']\n"
     ]
    }
   ],
   "source": [
    "l = 100\n",
    "n = 1000\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "S = [\"\".join(rng.choice(['0','1'],l).tolist()) for i in range(n)]\n",
    "min = float('inf')\n",
    "\n",
    "#print(S[0])\n",
    "\n",
    "for s1,s2 in itertools.permutations(S, 2):\n",
    "    con = s1+s2\n",
    "    hash = int(str(hashlib.sha1(con.encode()).hexdigest()), 16)\n",
    "    \n",
    "    if hash < min:\n",
    "        min = hash\n",
    "        pair = [s1,s2]\n",
    "\n",
    "print(min,pair)\n",
    "\n",
    "# 34200239543057730708897409415012917584018 \n",
    "# ['1100011101100001001010110010000001110100000010001111011100111101010100010000011001100111001111101111', \n",
    "# '1000001100100000111011111000011011100000100000110000011010111111110101110001011001000101100110001111']\n",
    "\n",
    "#print(str(hashlib.sha1(S[0].encode()).hexdigest()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts 0 and 1': 29, 'texts 0 and 2': 11, 'texts 1 and 2': 16}\n"
     ]
    }
   ],
   "source": [
    "S3 = [{word for word in texts[i] if len(word)<8} for i in range(3)]\n",
    "\n",
    "hashes = [[]]\n",
    "\n",
    "row_indices = range(len(S3))\n",
    "combinations = list(itertools.combinations(row_indices, 2))\n",
    "row_pairs = [(S[i], S[j]) for i, j in combinations]\n",
    "\n",
    "for j in range(3):\n",
    "    hashes.append([])\n",
    "    for i in range(100):\n",
    "        hash_min = float('inf')\n",
    "        for element in S3[j]:\n",
    "            hash = int(str(hashlib.sha1(str(i).encode() + element.encode()).hexdigest()), 16)\n",
    "            if hash < hash_min:\n",
    "                hash_min = hash\n",
    "        hashes[j].append(hash_min)\n",
    "\n",
    "jaccard = {}\n",
    "\n",
    "for k, l in combinations:\n",
    "    jaccard[f\"texts {k} and {l}\"] = sum([int(hashes[k][i] == hashes[l][i]) for i in range(100)])    \n",
    "\n",
    "print(jaccard)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
